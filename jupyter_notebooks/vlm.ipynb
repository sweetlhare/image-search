{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# qwen-vl-utils transformers\n",
    "# !pip install -U git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hare/miniforge3/envs/torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from vision_process import process_vision_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка модели и процессора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.68s/it]\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", \n",
    "    # \"models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/\",\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    # device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции для обработки видео и ответов на вопросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_and_answer(image_path, question, max_new_tokens=128):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": image_path,\n",
    "                    \"max_pixels\": 360 * 420,\n",
    "                    \"fps\": 1.0,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    return output_text[0]\n",
    "\n",
    "def answer_multiple_choice(image_path, question, choices):\n",
    "    base_question = f\"{question}\\nChoices:\\n\"\n",
    "    for i, choice in enumerate(choices, 1):\n",
    "        base_question += f\"{i}. {choice}\\n\"\n",
    "    base_question += \"Please select the most appropriate answer by number.\"\n",
    "    \n",
    "    answer = process_video_and_answer(image_path, base_question)\n",
    "    try:\n",
    "        selected_number = int(answer.strip())\n",
    "        return choices[selected_number - 1]\n",
    "    except:\n",
    "        return answer  # Возвращаем полный ответ, если не удалось извлечь номер\n",
    "\n",
    "def generate_video_description(image_path):\n",
    "    return process_video_and_answer(image_path, \"Describe this video in detail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset):\n",
    "    results = []\n",
    "    for item in dataset:\n",
    "        task_id = item['task_id']\n",
    "        task_type = item['task_type']\n",
    "        image_path = item['image']\n",
    "        \n",
    "        if task_type == 'qa':\n",
    "            question = item['question']\n",
    "            choices = [choice['choice'] for choice in item['choices']]\n",
    "            answer = answer_multiple_choice(image_path, question, choices)\n",
    "        elif task_type == 'captioning':\n",
    "            answer = generate_video_description(image_path)\n",
    "        \n",
    "        results.append({\n",
    "            'task_id': task_id,\n",
    "            'answer': answer\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: 1\n",
      "Answer: Accordion\n",
      "\n",
      "Task ID: 2\n",
      "Answer: The video depicts a street scene in Berlin, featuring a man playing an accordion. The man is positioned in the foreground, smiling and looking towards the camera. He is wearing a flat cap and a suit, and he is holding an accordion with both hands. The accordion has a traditional design with a large, rectangular body and a series of buttons along the top edge.\n",
      "\n",
      "In the background, there are several people sitting on benches, engaged in conversation or enjoying the surroundings. The setting appears to be a public square or a pedestrian area, with trees lining the street and a prominent structure in the background that resembles the Brandenburg Gate, a famous landmark\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Пример датасета (замените на реальные данные)\n",
    "sample_dataset = [\n",
    "    {\n",
    "        'task_id': 1,\n",
    "        'task_type': 'qa',\n",
    "        'image': '0b9649438a916859.jpg',\n",
    "        'question': 'What on the image?',\n",
    "        'choices': [\n",
    "            {'choice_id': 1, 'choice': 'Accordion'},\n",
    "            {'choice_id': 2, 'choice': 'Brokkoli'},\n",
    "            {'choice_id': 3, 'choice': 'Hat'},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'task_id': 2,\n",
    "        'task_type': 'captioning',\n",
    "        'image': '0b9649438a916859.jpg',\n",
    "    }\n",
    "]\n",
    "\n",
    "results = process_dataset(sample_dataset)\n",
    "for result in results:\n",
    "    print(f\"Task ID: {result['task_id']}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Insect',\n",
       " 'Hippopotamus',\n",
       " 'Spatula',\n",
       " 'Apple',\n",
       " 'Helmet',\n",
       " 'Skull',\n",
       " 'Lipstick',\n",
       " 'Sparrow',\n",
       " 'Scarf',\n",
       " 'Jet ski',\n",
       " 'Magpie',\n",
       " 'Cat',\n",
       " 'Rhinoceros',\n",
       " 'Pancake',\n",
       " 'Limousine',\n",
       " 'Ant',\n",
       " 'Jacket',\n",
       " 'Seahorse',\n",
       " 'Pear',\n",
       " 'Piano',\n",
       " 'Cello',\n",
       " '.DS_Store',\n",
       " 'Frying pan',\n",
       " 'Aircraft',\n",
       " 'Belt',\n",
       " 'Bow and arrow',\n",
       " 'Wrench',\n",
       " 'Alarm clock',\n",
       " 'Wok',\n",
       " 'Microwave oven',\n",
       " 'Goldfish',\n",
       " 'Whiteboard',\n",
       " 'Wine rack',\n",
       " 'Harp',\n",
       " 'Accordion',\n",
       " 'Zebra',\n",
       " 'Camera',\n",
       " 'Cucumber',\n",
       " 'Alpaca',\n",
       " 'Wheel',\n",
       " 'Cosmetics',\n",
       " 'Honeycomb',\n",
       " 'Ambulance',\n",
       " 'Fedora',\n",
       " 'Goat',\n",
       " 'Lily',\n",
       " 'Toilet paper',\n",
       " 'Parking meter',\n",
       " 'Tap',\n",
       " 'Earrings',\n",
       " 'Vase',\n",
       " 'Glasses',\n",
       " 'Submarine',\n",
       " 'Snowboard',\n",
       " 'Christmas tree',\n",
       " 'Cassette deck',\n",
       " 'Tea',\n",
       " 'Glove',\n",
       " 'Coin',\n",
       " 'Woodpecker',\n",
       " 'Airplane',\n",
       " 'Ipod',\n",
       " 'Worm',\n",
       " 'Animal',\n",
       " 'Binoculars',\n",
       " 'Isopod',\n",
       " 'Invertebrate',\n",
       " 'Monkey',\n",
       " 'Whisk',\n",
       " 'Flashlight',\n",
       " 'Broccoli',\n",
       " 'Sombrero',\n",
       " 'Spoon',\n",
       " 'Plastic bag',\n",
       " 'Adhesive tape',\n",
       " 'Bread',\n",
       " 'Lighthouse',\n",
       " 'Hat',\n",
       " 'Rabbit',\n",
       " 'Artichoke',\n",
       " 'Bathtub',\n",
       " 'Reptile',\n",
       " 'Tablet computer',\n",
       " 'Koala',\n",
       " 'High heels',\n",
       " 'Briefcase',\n",
       " 'Cannon',\n",
       " 'Rocket',\n",
       " 'Barge',\n",
       " 'Common fig',\n",
       " 'Banana',\n",
       " 'Shelf',\n",
       " 'Mixer',\n",
       " 'Cutting board',\n",
       " 'Grape',\n",
       " 'Laptop',\n",
       " 'Zucchini',\n",
       " 'Wood-burning stove',\n",
       " 'Winter melon',\n",
       " 'Horse',\n",
       " 'Bottle',\n",
       " 'Lion',\n",
       " 'Woman',\n",
       " 'Elephant',\n",
       " 'Willow',\n",
       " 'Land vehicle']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen-VL model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:10<00:00,  5.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cpu!\n",
      "Processing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/106 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:17, 17.71s/it]\u001b[A\n",
      "2it [00:32, 15.92s/it]\u001b[A\n",
      "3it [00:46, 15.01s/it]\u001b[A\n",
      "4it [00:58, 13.82s/it]\u001b[A\n",
      "5it [01:14, 14.61s/it]\u001b[A\n",
      "6it [01:29, 14.90s/it]\u001b[A\n",
      "7it [01:42, 14.22s/it]\u001b[A\n",
      "8it [01:57, 14.29s/it]\u001b[A\n",
      "9it [02:14, 15.34s/it]\u001b[A\n",
      "10it [02:25, 13.96s/it]\u001b[A\n",
      "11it [02:54, 18.54s/it]\u001b[A\n",
      "12it [03:17, 19.95s/it]\u001b[A\n",
      "13it [03:28, 17.30s/it]\u001b[A\n",
      "14it [03:45, 17.01s/it]\u001b[A\n",
      "15it [04:01, 16.73s/it]\u001b[A\n",
      "16it [04:17, 16.61s/it]\u001b[A\n",
      "17it [04:36, 17.22s/it]\u001b[A\n",
      "18it [04:47, 15.36s/it]\u001b[A\n",
      "19it [05:03, 15.67s/it]\u001b[A\n",
      "20it [05:18, 15.42s/it]\u001b[A\n",
      "21it [05:35, 15.94s/it]\u001b[A\n",
      "22it [05:57, 17.67s/it]\u001b[A\n",
      "23it [06:03, 14.06s/it]\u001b[A\n",
      "24it [06:19, 14.76s/it]\u001b[A\n",
      "25it [06:31, 13.82s/it]\u001b[A\n",
      "26it [06:45, 14.02s/it]\u001b[A\n",
      "27it [06:59, 13.90s/it]\u001b[A\n",
      "28it [07:27, 18.28s/it]\u001b[A\n",
      "29it [07:41, 16.97s/it]\u001b[A\n",
      "30it [07:52, 15.16s/it]\u001b[A\n",
      "31it [08:21, 19.24s/it]\u001b[A\n",
      "32it [08:32, 16.83s/it]\u001b[A\n",
      "33it [08:49, 17.04s/it]\u001b[A\n",
      "34it [09:01, 15.25s/it]\u001b[A\n",
      "35it [09:14, 14.58s/it]\u001b[A\n",
      "36it [09:25, 13.58s/it]\u001b[A\n",
      "37it [09:41, 14.40s/it]\u001b[A\n",
      "38it [09:52, 13.25s/it]\u001b[A\n",
      "39it [10:08, 14.28s/it]\u001b[A\n",
      "40it [10:22, 14.07s/it]\u001b[A\n",
      "41it [10:31, 12.69s/it]\u001b[A\n",
      "42it [10:45, 12.87s/it]\u001b[A\n",
      "43it [10:57, 12.57s/it]\u001b[A\n",
      "44it [11:10, 12.78s/it]\u001b[A\n",
      "45it [11:20, 12.13s/it]\u001b[A\n",
      "46it [11:39, 14.17s/it]\u001b[A\n",
      "47it [11:53, 14.00s/it]\u001b[A\n",
      "48it [12:07, 13.86s/it]\u001b[A\n",
      "49it [12:20, 13.71s/it]\u001b[A\n",
      "50it [12:29, 12.27s/it]\u001b[A\n",
      "51it [12:42, 12.48s/it]\u001b[A\n",
      "52it [12:47, 10.23s/it]\u001b[A\n",
      "53it [12:52,  8.88s/it]\u001b[A\n",
      "54it [13:07, 10.56s/it]\u001b[A\n",
      "55it [13:30, 14.21s/it]\u001b[A\n",
      "56it [13:41, 13.28s/it]\u001b[A\n",
      "57it [13:50, 12.16s/it]\u001b[A\n",
      "58it [14:04, 12.54s/it]\u001b[A\n",
      "59it [14:17, 12.77s/it]\u001b[A\n",
      "60it [14:38, 15.19s/it]\u001b[A\n",
      "61it [14:58, 16.62s/it]\u001b[A\n",
      "62it [15:07, 14.41s/it]\u001b[A\n",
      "63it [15:20, 14.07s/it]\u001b[A\n",
      "64it [15:34, 14.03s/it]\u001b[A\n",
      "65it [15:48, 13.80s/it]\u001b[A\n",
      "66it [16:03, 14.34s/it]\u001b[A\n",
      "67it [16:15, 13.62s/it]\u001b[A\n",
      "68it [16:25, 12.43s/it]\u001b[A\n",
      "69it [16:35, 11.89s/it]\u001b[A\n",
      "70it [16:50, 12.63s/it]\u001b[A\n",
      "71it [17:01, 12.08s/it]\u001b[A\n",
      "72it [17:12, 11.84s/it]\u001b[A\n",
      "73it [17:25, 12.20s/it]\u001b[A\n",
      "74it [17:38, 12.62s/it]\u001b[A\n",
      "75it [17:52, 12.89s/it]\u001b[A\n",
      "76it [18:04, 12.68s/it]\u001b[A\n",
      "77it [18:13, 11.61s/it]\u001b[A\n",
      "78it [18:26, 11.83s/it]\u001b[A\n",
      "79it [18:51, 15.99s/it]\u001b[A\n",
      "80it [19:03, 14.60s/it]\u001b[A\n",
      "81it [19:14, 13.69s/it]\u001b[A\n",
      "82it [19:30, 14.37s/it]\u001b[A\n",
      "83it [19:40, 13.12s/it]\u001b[A\n",
      "84it [19:52, 12.58s/it]\u001b[A\n",
      "85it [20:04, 12.41s/it]\u001b[A\n",
      "86it [20:13, 11.39s/it]\u001b[A\n",
      "87it [20:27, 12.10s/it]\u001b[A\n",
      "88it [20:45, 14.02s/it]\u001b[A\n",
      "89it [20:55, 12.88s/it]\u001b[A\n",
      "90it [21:09, 13.00s/it]\u001b[A\n",
      "91it [21:22, 13.05s/it]\u001b[A\n",
      "92it [21:44, 15.93s/it]\u001b[A\n",
      "93it [22:07, 17.98s/it]\u001b[A\n",
      "94it [22:21, 16.85s/it]\u001b[A\n",
      "95it [22:46, 19.25s/it]\u001b[A\n",
      "96it [22:54, 15.95s/it]\u001b[A\n",
      "97it [23:09, 15.40s/it]\u001b[A\n",
      "98it [23:19, 14.06s/it]\u001b[A\n",
      "99it [23:40, 16.03s/it]\u001b[A\n",
      "100it [23:55, 15.74s/it]\u001b[A\n",
      "101it [24:06, 14.23s/it]\u001b[A\n",
      "102it [24:17, 13.17s/it]\u001b[A\n",
      "103it [24:32, 13.83s/it]\u001b[A\n",
      "104it [24:48, 14.39s/it]\u001b[A\n",
      "105it [24:59, 13.58s/it]\u001b[A\n",
      "106it [25:10, 12.69s/it]\u001b[A\n",
      "107it [25:23, 12.82s/it]\u001b[A\n",
      "108it [25:40, 14.15s/it]\u001b[A\n",
      "109it [25:59, 15.39s/it]\u001b[A\n",
      "110it [26:14, 15.47s/it]\u001b[A\n",
      "111it [26:30, 15.61s/it]\u001b[A\n",
      "112it [26:55, 18.27s/it]\u001b[A\n",
      "113it [27:06, 16.08s/it]\u001b[A\n",
      "114it [27:19, 15.28s/it]\u001b[A\n",
      "115it [27:27, 13.15s/it]\u001b[A\n",
      "116it [27:38, 12.46s/it]\u001b[A\n",
      "117it [27:52, 12.75s/it]\u001b[A\n",
      "118it [27:58, 10.73s/it]\u001b[A\n",
      "119it [28:10, 11.13s/it]\u001b[A\n",
      "120it [28:33, 14.93s/it]\u001b[A\n",
      "121it [28:45, 13.90s/it]\u001b[A\n",
      "122it [28:59, 13.94s/it]\u001b[A\n",
      "123it [29:17, 15.26s/it]\u001b[A\n",
      "124it [29:42, 18.18s/it]\u001b[A\n",
      "125it [29:53, 15.89s/it]\u001b[A\n",
      "126it [30:04, 14.45s/it]\u001b[A\n",
      "127it [30:18, 14.33s/it]\u001b[A\n",
      "128it [30:41, 16.91s/it]\u001b[A\n",
      "129it [30:55, 16.02s/it]\u001b[A\n",
      "130it [31:05, 14.11s/it]\u001b[A\n",
      "131it [31:28, 17.05s/it]\u001b[A\n",
      "132it [31:39, 15.19s/it]\u001b[A\n",
      "133it [31:52, 14.44s/it]\u001b[A\n",
      "134it [32:06, 14.31s/it]\u001b[A\n",
      "135it [32:17, 13.23s/it]\u001b[A\n",
      "136it [32:30, 13.22s/it]\u001b[A\n",
      "137it [32:53, 16.32s/it]\u001b[A\n",
      "138it [33:06, 15.29s/it]\u001b[A\n",
      "139it [33:11, 12.01s/it]\u001b[A\n",
      "140it [33:21, 11.65s/it]\u001b[A\n",
      "141it [33:35, 12.23s/it]\u001b[A\n",
      "142it [34:03, 17.00s/it]\u001b[A\n",
      "143it [34:15, 15.30s/it]\u001b[A\n",
      "144it [34:36, 17.02s/it]\u001b[A\n",
      "145it [34:49, 15.89s/it]\u001b[A\n",
      "146it [35:12, 18.00s/it]\u001b[A\n",
      "147it [35:23, 15.96s/it]\u001b[A\n",
      "148it [35:34, 14.44s/it]\u001b[A\n",
      "149it [35:45, 13.34s/it]\u001b[A\n",
      "150it [35:59, 13.65s/it]\u001b[A\n",
      "151it [36:21, 16.25s/it]\u001b[A\n",
      "152it [36:28, 13.36s/it]\u001b[A\n",
      "153it [36:42, 13.59s/it]\u001b[A\n",
      "154it [36:55, 13.53s/it]\u001b[A\n",
      "155it [37:08, 13.16s/it]\u001b[A\n",
      "156it [37:18, 12.36s/it]\u001b[A\n",
      "157it [37:29, 11.88s/it]\u001b[A\n",
      "158it [37:40, 11.55s/it]\u001b[A\n",
      "159it [37:52, 11.63s/it]\u001b[A\n",
      "160it [38:07, 12.80s/it]\u001b[A\n",
      "161it [38:31, 16.25s/it]\u001b[A\n",
      "162it [38:42, 14.62s/it]\u001b[A\n",
      "163it [38:53, 13.46s/it]\u001b[A\n",
      "164it [39:04, 12.64s/it]\u001b[A\n",
      "165it [39:18, 13.08s/it]\u001b[A\n",
      "166it [39:29, 12.56s/it]\u001b[A\n",
      "167it [39:54, 16.36s/it]\u001b[A\n",
      "168it [40:08, 15.43s/it]\u001b[A\n",
      "169it [40:18, 13.98s/it]\u001b[A\n",
      "170it [40:32, 13.87s/it]\u001b[A\n",
      "171it [40:43, 12.90s/it]\u001b[A\n",
      "172it [40:59, 13.91s/it]\u001b[A\n",
      "173it [41:19, 15.88s/it]\u001b[A\n",
      "174it [41:33, 15.33s/it]\u001b[A\n",
      "175it [41:47, 14.75s/it]\u001b[A\n",
      "176it [41:57, 13.47s/it]\u001b[A\n",
      "177it [42:10, 13.33s/it]\u001b[A\n",
      "178it [42:22, 12.90s/it]\u001b[A\n",
      "179it [42:33, 12.21s/it]\u001b[A\n",
      "180it [42:50, 13.66s/it]\u001b[A\n",
      "181it [43:05, 14.22s/it]\u001b[A\n",
      "182it [43:18, 13.86s/it]\u001b[A\n",
      "183it [43:31, 13.60s/it]\u001b[A\n",
      "184it [43:45, 13.51s/it]\u001b[A\n",
      "185it [43:55, 12.67s/it]\u001b[A\n",
      "186it [44:05, 11.85s/it]\u001b[A\n",
      "187it [44:18, 12.20s/it]\u001b[A\n",
      "188it [44:41, 15.43s/it]\u001b[A\n",
      "189it [44:55, 14.93s/it]\u001b[A\n",
      "190it [45:08, 14.38s/it]\u001b[A\n",
      "191it [45:24, 14.95s/it]\u001b[A\n",
      "192it [45:35, 13.59s/it]\u001b[A\n",
      "193it [45:45, 12.51s/it]\u001b[A\n",
      "194it [45:58, 12.63s/it]\u001b[A\n",
      "195it [46:11, 12.72s/it]\u001b[A\n",
      "196it [46:23, 12.78s/it]\u001b[A\n",
      "197it [46:37, 13.08s/it]\u001b[A\n",
      "198it [46:47, 11.96s/it]\u001b[A\n",
      "199it [46:57, 11.54s/it]\u001b[A\n",
      "200it [47:08, 14.14s/it]\u001b[A\n",
      "  1%|▎                                     | 1/106 [47:08<82:29:13, 2828.13s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:10, 10.64s/it]\u001b[A\n",
      "2it [00:23, 12.09s/it]\u001b[A\n",
      "3it [00:36, 12.49s/it]\u001b[A\n",
      "4it [00:46, 11.62s/it]\u001b[A\n",
      "5it [00:57, 11.12s/it]\u001b[A\n",
      "6it [01:07, 10.85s/it]\u001b[A\n",
      "7it [01:20, 11.45s/it]\u001b[A\n",
      "8it [01:30, 11.08s/it]\u001b[A\n",
      "9it [01:40, 10.87s/it]\u001b[A\n",
      "10it [01:51, 10.70s/it]\u001b[A\n",
      "11it [02:04, 11.42s/it]\u001b[A\n",
      "12it [02:17, 11.89s/it]\u001b[A\n",
      "13it [02:30, 12.22s/it]\u001b[A\n",
      "14it [02:39, 11.36s/it]\u001b[A\n",
      "15it [02:50, 11.10s/it]\u001b[A\n",
      "16it [03:01, 11.23s/it]\u001b[A\n",
      "17it [03:15, 11.87s/it]\u001b[A\n",
      "18it [03:25, 11.48s/it]\u001b[A\n",
      "19it [03:36, 11.17s/it]\u001b[A\n",
      "20it [03:48, 11.70s/it]\u001b[A\n",
      "21it [04:01, 12.09s/it]\u001b[A\n",
      "22it [04:12, 11.62s/it]\u001b[A\n",
      "23it [04:26, 12.34s/it]\u001b[A\n",
      "24it [04:36, 11.77s/it]\u001b[A\n",
      "25it [04:47, 11.40s/it]\u001b[A\n",
      "26it [05:00, 11.88s/it]\u001b[A\n",
      "27it [05:09, 11.15s/it]\u001b[A\n",
      "28it [05:20, 10.98s/it]\u001b[A\n",
      "29it [05:34, 11.84s/it]\u001b[A\n",
      "30it [05:44, 11.47s/it]\u001b[A\n",
      "31it [05:58, 11.97s/it]\u001b[A\n",
      "32it [06:08, 11.53s/it]\u001b[A\n",
      "33it [06:19, 11.22s/it]\u001b[A\n",
      "34it [06:25,  9.63s/it]\u001b[A\n",
      "35it [06:38, 10.79s/it]\u001b[A\n",
      "36it [06:51, 11.47s/it]\u001b[A\n",
      "37it [07:02, 11.21s/it]\u001b[A\n",
      "38it [07:22, 13.96s/it]\u001b[A\n",
      "39it [07:33, 12.94s/it]\u001b[A\n",
      "40it [07:43, 12.19s/it]\u001b[A\n",
      "41it [07:54, 11.69s/it]\u001b[A\n",
      "42it [08:04, 11.35s/it]\u001b[A\n",
      "43it [08:17, 11.83s/it]\u001b[A\n",
      "44it [08:28, 11.42s/it]\u001b[A\n",
      "45it [08:38, 11.15s/it]\u001b[A\n",
      "46it [08:49, 10.99s/it]\u001b[A\n",
      "47it [09:02, 11.60s/it]\u001b[A\n",
      "48it [09:15, 12.24s/it]\u001b[A\n",
      "49it [09:41, 16.27s/it]\u001b[A\n",
      "50it [09:55, 15.45s/it]\u001b[A\n",
      "51it [10:08, 14.72s/it]\u001b[A\n",
      "52it [10:24, 15.06s/it]\u001b[A\n",
      "53it [10:34, 13.79s/it]\u001b[A\n",
      "54it [10:46, 13.24s/it]\u001b[A\n",
      "55it [11:13, 17.36s/it]\u001b[A\n",
      "56it [11:24, 15.39s/it]\u001b[A\n",
      "57it [11:33, 13.57s/it]\u001b[A\n",
      "58it [11:47, 13.59s/it]\u001b[A\n",
      "59it [11:58, 12.70s/it]\u001b[A\n",
      "60it [12:04, 10.80s/it]\u001b[A\n",
      "61it [12:18, 11.75s/it]\u001b[A\n",
      "62it [12:31, 12.18s/it]\u001b[A\n",
      "63it [12:48, 13.53s/it]\u001b[A\n",
      "64it [12:58, 12.64s/it]\u001b[A\n",
      "65it [13:11, 12.77s/it]\u001b[A\n",
      "66it [13:22, 12.15s/it]\u001b[A\n",
      "67it [13:35, 12.42s/it]\u001b[A\n",
      "68it [13:48, 12.60s/it]\u001b[A\n",
      "69it [13:59, 11.99s/it]\u001b[A\n",
      "70it [14:09, 11.40s/it]\u001b[A\n",
      "71it [14:19, 11.16s/it]\u001b[A\n",
      "72it [14:30, 11.00s/it]\u001b[A\n",
      "73it [14:41, 10.87s/it]\u001b[A\n",
      "74it [14:54, 11.66s/it]\u001b[A\n",
      "75it [15:05, 11.33s/it]\u001b[A\n",
      "76it [15:15, 11.11s/it]\u001b[A\n",
      "77it [15:28, 11.70s/it]\u001b[A\n",
      "78it [15:43, 12.47s/it]\u001b[A\n",
      "79it [15:53, 11.92s/it]\u001b[A\n",
      "80it [16:02, 11.01s/it]\u001b[A\n",
      "81it [16:16, 11.71s/it]\u001b[A\n",
      "82it [16:26, 11.38s/it]\u001b[A\n",
      "83it [16:35, 10.64s/it]\u001b[A\n",
      "84it [16:46, 10.69s/it]\u001b[A\n",
      "85it [17:02, 12.21s/it]\u001b[A\n",
      "86it [17:12, 11.76s/it]\u001b[A\n",
      "87it [17:27, 12.76s/it]\u001b[A\n",
      "88it [17:41, 13.00s/it]\u001b[A\n",
      "89it [17:54, 13.04s/it]\u001b[A\n",
      "90it [18:05, 12.31s/it]\u001b[A\n",
      "91it [18:18, 12.56s/it]\u001b[A\n",
      "92it [18:28, 11.98s/it]\u001b[A\n",
      "93it [18:38, 11.24s/it]\u001b[A\n",
      "94it [18:49, 11.06s/it]\u001b[A\n",
      "95it [19:02, 11.70s/it]\u001b[A\n",
      "96it [19:13, 11.67s/it]\u001b[A\n",
      "97it [19:24, 11.34s/it]\u001b[A\n",
      "98it [19:35, 11.12s/it]\u001b[A\n",
      "99it [19:48, 11.84s/it]\u001b[A\n",
      "100it [19:59, 11.51s/it]\u001b[A\n",
      "101it [20:10, 11.37s/it]\u001b[A\n",
      "102it [20:21, 11.25s/it]\u001b[A\n",
      "103it [20:31, 11.05s/it]\u001b[A\n",
      "104it [20:46, 11.97s/it]\u001b[A\n",
      "105it [20:56, 11.56s/it]\u001b[A\n",
      "106it [21:07, 11.25s/it]\u001b[A\n",
      "107it [21:20, 11.87s/it]\u001b[A\n",
      "108it [21:43, 15.31s/it]\u001b[A\n",
      "109it [21:52, 13.41s/it]\u001b[A\n",
      "110it [22:03, 12.59s/it]\u001b[A\n",
      "111it [22:13, 11.88s/it]\u001b[A\n",
      "112it [22:24, 11.54s/it]\u001b[A\n",
      "113it [22:38, 12.13s/it]\u001b[A\n",
      "114it [22:51, 12.66s/it]\u001b[A\n",
      "115it [23:05, 12.86s/it]\u001b[A\n",
      "116it [23:14, 11.68s/it]\u001b[A\n",
      "117it [23:27, 12.09s/it]\u001b[A\n",
      "118it [23:32, 10.14s/it]\u001b[A\n",
      "119it [23:43, 10.31s/it]\u001b[A\n",
      "120it [24:06, 14.23s/it]\u001b[A\n",
      "121it [24:17, 13.13s/it]\u001b[A\n",
      "122it [24:28, 12.36s/it]\u001b[A\n",
      "123it [24:36, 11.32s/it]\u001b[A\n",
      "124it [24:47, 11.08s/it]\u001b[A\n",
      "125it [24:58, 10.93s/it]\u001b[A\n",
      "126it [25:11, 11.58s/it]\u001b[A\n",
      "127it [25:21, 11.30s/it]\u001b[A\n",
      "128it [25:32, 11.09s/it]\u001b[A\n",
      "129it [25:43, 10.96s/it]\u001b[A\n",
      "130it [25:53, 10.84s/it]\u001b[A\n",
      "131it [26:02, 10.29s/it]\u001b[A\n",
      "132it [26:13, 10.39s/it]\u001b[A\n",
      "133it [26:24, 10.52s/it]\u001b[A\n",
      "134it [26:32,  9.83s/it]\u001b[A\n",
      "135it [26:42, 10.07s/it]\u001b[A\n",
      "136it [27:06, 14.16s/it]\u001b[A\n",
      "137it [27:20, 13.94s/it]\u001b[A\n",
      "138it [27:44, 16.96s/it]\u001b[A\n",
      "139it [27:57, 15.94s/it]\u001b[A\n",
      "140it [28:08, 14.44s/it]\u001b[A\n",
      "141it [28:23, 14.71s/it]\u001b[A\n",
      "142it [28:48, 17.78s/it]\u001b[A\n",
      "143it [29:00, 15.87s/it]\u001b[A\n",
      "144it [29:09, 13.81s/it]\u001b[A\n",
      "145it [29:22, 13.61s/it]\u001b[A\n",
      "146it [29:33, 12.79s/it]\u001b[A\n",
      "147it [29:44, 12.20s/it]\u001b[A\n",
      "148it [29:57, 12.54s/it]\u001b[A\n",
      "149it [30:10, 12.75s/it]\u001b[A\n",
      "150it [30:21, 12.08s/it]\u001b[A\n",
      "151it [30:30, 11.17s/it]\u001b[A\n",
      "152it [30:40, 11.05s/it]\u001b[A\n",
      "153it [30:51, 10.91s/it]\u001b[A\n",
      "154it [31:01, 10.76s/it]\u001b[A\n",
      "155it [31:14, 11.33s/it]\u001b[A\n",
      "156it [31:26, 11.38s/it]\u001b[A\n",
      "157it [31:39, 12.06s/it]\u001b[A\n",
      "158it [31:53, 12.56s/it]\u001b[A\n",
      "159it [32:04, 12.04s/it]\u001b[A\n",
      "160it [32:15, 11.65s/it]\u001b[A\n",
      "161it [32:25, 11.40s/it]\u001b[A\n",
      "162it [32:52, 15.95s/it]\u001b[A\n",
      "163it [32:56, 12.52s/it]\u001b[A\n",
      "164it [33:08, 12.18s/it]\u001b[A\n",
      "165it [33:19, 11.87s/it]\u001b[A\n",
      "166it [33:32, 12.35s/it]\u001b[A\n",
      "167it [33:43, 11.85s/it]\u001b[A\n",
      "168it [33:56, 12.21s/it]\u001b[A\n",
      "169it [34:07, 11.70s/it]\u001b[A\n",
      "170it [34:16, 10.87s/it]\u001b[A\n",
      "171it [34:26, 10.77s/it]\u001b[A\n",
      "172it [34:36, 10.52s/it]\u001b[A\n",
      "173it [34:47, 10.55s/it]\u001b[A\n",
      "174it [35:00, 11.27s/it]\u001b[A\n",
      "175it [35:10, 11.06s/it]\u001b[A\n",
      "176it [35:21, 10.91s/it]\u001b[A\n",
      "177it [35:34, 11.55s/it]\u001b[A\n",
      "178it [35:44, 11.27s/it]\u001b[A\n",
      "179it [36:10, 15.43s/it]\u001b[A\n",
      "180it [36:20, 13.97s/it]\u001b[A\n",
      "181it [36:36, 14.49s/it]\u001b[A\n",
      "182it [36:49, 14.03s/it]\u001b[A\n",
      "183it [37:02, 13.75s/it]\u001b[A\n",
      "184it [37:15, 13.61s/it]\u001b[A\n",
      "185it [37:26, 12.68s/it]\u001b[A\n",
      "186it [37:37, 12.13s/it]\u001b[A\n",
      "187it [37:50, 12.49s/it]\u001b[A\n",
      "188it [38:01, 11.94s/it]\u001b[A\n",
      "189it [38:10, 11.06s/it]\u001b[A\n",
      "190it [38:23, 11.77s/it]\u001b[A\n",
      "191it [38:34, 11.49s/it]\u001b[A\n",
      "192it [38:44, 11.20s/it]\u001b[A\n",
      "193it [38:55, 11.01s/it]\u001b[A\n",
      "194it [39:09, 12.03s/it]\u001b[A\n",
      "195it [39:23, 12.47s/it]\u001b[A\n",
      "196it [39:33, 11.89s/it]\u001b[A\n",
      "197it [39:46, 12.20s/it]\u001b[A\n",
      "198it [39:55, 11.19s/it]\u001b[A\n",
      "199it [40:04, 10.53s/it]\u001b[A\n",
      "200it [40:15, 12.08s/it]\u001b[A\n",
      "  2%|▋                                   | 2/106 [1:27:23<74:41:04, 2585.24s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:13, 13.15s/it]\u001b[A\n",
      "2it [00:24, 12.08s/it]\u001b[A\n",
      "3it [00:35, 11.52s/it]\u001b[A\n",
      "4it [00:52, 13.64s/it]\u001b[A\n",
      "5it [01:03, 12.92s/it]\u001b[A\n",
      "6it [01:15, 12.38s/it]\u001b[A\n",
      "7it [01:28, 12.76s/it]\u001b[A\n",
      "8it [01:44, 13.61s/it]\u001b[A\n",
      "9it [01:54, 12.71s/it]\u001b[A\n",
      "10it [02:05, 12.05s/it]\u001b[A\n",
      "11it [02:18, 12.35s/it]\u001b[A\n",
      "12it [02:29, 11.80s/it]\u001b[A\n",
      "13it [02:43, 12.52s/it]\u001b[A\n",
      "14it [02:53, 11.94s/it]\u001b[A\n",
      "15it [03:07, 12.32s/it]\u001b[A\n",
      "16it [03:17, 11.78s/it]\u001b[A\n",
      "17it [03:30, 12.15s/it]\u001b[A\n",
      "18it [03:41, 11.67s/it]\u001b[A\n",
      "19it [03:52, 11.61s/it]\u001b[A\n",
      "20it [04:05, 12.04s/it]\u001b[A\n",
      "21it [04:16, 11.58s/it]\u001b[A\n",
      "22it [04:26, 11.25s/it]\u001b[A\n",
      "23it [04:37, 11.03s/it]\u001b[A\n",
      "24it [04:59, 14.48s/it]\u001b[A\n",
      "25it [05:12, 14.10s/it]\u001b[A\n",
      "26it [05:25, 13.75s/it]\u001b[A\n",
      "27it [05:38, 13.54s/it]\u001b[A\n",
      "28it [05:51, 13.36s/it]\u001b[A\n",
      "29it [06:04, 13.25s/it]\u001b[A\n",
      "30it [06:17, 13.16s/it]\u001b[A\n",
      "31it [06:28, 12.35s/it]\u001b[A\n",
      "32it [06:50, 15.44s/it]\u001b[A\n",
      "33it [07:03, 14.73s/it]\u001b[A\n",
      "34it [07:14, 13.45s/it]\u001b[A\n",
      "35it [07:24, 12.56s/it]\u001b[A\n",
      "36it [07:47, 15.54s/it]\u001b[A\n",
      "37it [07:55, 13.32s/it]\u001b[A\n",
      "38it [08:19, 13.15s/it]\u001b[A\n",
      "  3%|█                                   | 3/106 [1:35:43<46:43:20, 1633.02s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:13, 13.73s/it]\u001b[A\n",
      "2it [00:21, 10.48s/it]\u001b[A\n",
      "3it [00:33, 11.14s/it]\u001b[A\n",
      "4it [00:44, 10.81s/it]\u001b[A\n",
      "5it [00:55, 10.86s/it]\u001b[A\n",
      "6it [01:07, 11.28s/it]\u001b[A\n",
      "7it [01:18, 11.42s/it]\u001b[A\n",
      "8it [01:34, 12.71s/it]\u001b[A\n",
      "9it [01:47, 12.87s/it]\u001b[A\n",
      "10it [02:00, 12.92s/it]\u001b[A\n",
      "11it [02:13, 13.02s/it]\u001b[A\n",
      "12it [02:24, 12.29s/it]\u001b[A\n",
      "13it [02:37, 12.50s/it]\u001b[A\n",
      "14it [02:52, 13.39s/it]\u001b[A\n",
      "15it [03:15, 16.27s/it]\u001b[A\n",
      "16it [03:29, 15.35s/it]\u001b[A\n",
      "17it [03:42, 14.66s/it]\u001b[A\n",
      "18it [03:52, 13.44s/it]\u001b[A\n",
      "19it [04:04, 13.07s/it]\u001b[A\n",
      "20it [04:22, 14.32s/it]\u001b[A\n",
      "21it [04:34, 13.63s/it]\u001b[A\n",
      "22it [04:47, 13.47s/it]\u001b[A\n",
      "23it [04:57, 12.60s/it]\u001b[A\n",
      "24it [05:21, 16.01s/it]\u001b[A\n",
      "25it [05:35, 15.20s/it]\u001b[A\n",
      "26it [05:45, 13.84s/it]\u001b[A\n",
      "27it [06:00, 14.00s/it]\u001b[A\n",
      "28it [06:10, 13.03s/it]\u001b[A\n",
      "29it [06:23, 13.01s/it]\u001b[A\n",
      "30it [06:46, 15.80s/it]\u001b[A\n",
      "31it [06:58, 14.73s/it]\u001b[A\n",
      "32it [07:11, 14.20s/it]\u001b[A\n",
      "33it [07:24, 13.91s/it]\u001b[A\n",
      "34it [07:37, 13.65s/it]\u001b[A\n",
      "35it [07:49, 13.21s/it]\u001b[A\n",
      "36it [08:00, 12.40s/it]\u001b[A\n",
      "37it [08:13, 12.55s/it]\u001b[A\n",
      "38it [08:38, 16.29s/it]\u001b[A\n",
      "39it [08:51, 15.33s/it]\u001b[A\n",
      "40it [09:01, 13.88s/it]\u001b[A\n",
      "41it [09:14, 13.58s/it]\u001b[A\n",
      "42it [09:25, 12.67s/it]\u001b[A\n",
      "43it [09:35, 12.04s/it]\u001b[A\n",
      "44it [09:58, 15.14s/it]\u001b[A\n",
      "45it [10:08, 13.77s/it]\u001b[A\n",
      "46it [10:21, 13.54s/it]\u001b[A\n",
      "47it [10:32, 12.61s/it]\u001b[A\n",
      "48it [10:42, 11.84s/it]\u001b[A\n",
      "49it [10:55, 12.19s/it]\u001b[A\n",
      "50it [11:05, 11.69s/it]\u001b[A\n",
      "51it [11:16, 11.33s/it]\u001b[A\n",
      "52it [11:26, 11.09s/it]\u001b[A\n",
      "53it [11:37, 10.92s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from annoy import AnnoyIndex\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "class VLMImageSearch:\n",
    "    def __init__(self):\n",
    "        print(\"Loading Qwen-VL model...\")\n",
    "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "        )\n",
    "        self.processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded successfully on {self.device}!\")\n",
    "\n",
    "    def get_embedding(self, image_path):\n",
    "        try:\n",
    "            # Загрузка и предобработка изображения\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            # Подготовка входных данных для модели\n",
    "            inputs = self.processor(\n",
    "                text=[\"Describe this image.\"],  # Используем простой промпт\n",
    "                images=[image],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Получаем эмбеддинги из последнего hidden state\n",
    "                outputs = self.model(**inputs, output_hidden_states=True)\n",
    "                # Берем hidden states последнего слоя vision encoder\n",
    "                vision_hidden_states = outputs.hidden_states[-1]\n",
    "                # Используем среднее значение по токенам как эмбеддинг изображения\n",
    "                embedding = vision_hidden_states.mean(dim=1).squeeze().cpu().numpy()\n",
    "                \n",
    "            # Нормализация эмбеддинга\n",
    "            embedding = embedding / np.linalg.norm(embedding)\n",
    "            return embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "def process_dataset(dataset_path, encoder, save_dir=\"./data\"):\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    embeddings_dict = {}\n",
    "    file_mapping = {}\n",
    "    class_mapping = {}\n",
    "    reverse_class_mapping = {}\n",
    "    class_stats = defaultdict(int)\n",
    "    \n",
    "    idx = 0\n",
    "    print(\"Processing dataset...\")\n",
    "    \n",
    "    for class_dir in tqdm(list(dataset_path.iterdir())):\n",
    "        if class_dir.is_dir() and not class_dir.name.startswith('.'):\n",
    "            class_name = class_dir.name\n",
    "            for image_file in tqdm(class_dir.glob(\"*.*\")):\n",
    "                if image_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                    embedding = encoder.get_embedding(str(image_file))\n",
    "                    if embedding is not None:\n",
    "                        embeddings_dict[idx] = embedding\n",
    "                        file_mapping[idx] = image_file.stem\n",
    "                        class_mapping[idx] = class_name\n",
    "                        reverse_class_mapping[image_file.stem] = class_name\n",
    "                        class_stats[class_name] += 1\n",
    "                        idx += 1\n",
    "    \n",
    "    print(f\"\\nTotal images processed: {idx}\")\n",
    "    print(\"\\nClass distribution:\")\n",
    "    for class_name, count in class_stats.items():\n",
    "        print(f\"{class_name}: {count} images\")\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with open(f\"{save_dir}/processed_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': embeddings_dict,\n",
    "            'file_mapping': file_mapping,\n",
    "            'class_mapping': class_mapping,\n",
    "            'reverse_class_mapping': reverse_class_mapping,\n",
    "            'class_stats': dict(class_stats)\n",
    "        }, f)\n",
    "    \n",
    "    return embeddings_dict, file_mapping, class_mapping, reverse_class_mapping\n",
    "\n",
    "def build_index(embeddings_dict, save_dir=\"./data\"):\n",
    "    first_embedding = next(iter(embeddings_dict.values()))\n",
    "    embedding_dim = len(first_embedding)\n",
    "    \n",
    "    index = AnnoyIndex(embedding_dim, 'angular')\n",
    "    \n",
    "    print(\"Building index...\")\n",
    "    for idx, embedding in embeddings_dict.items():\n",
    "        index.add_item(idx, embedding)\n",
    "    \n",
    "    print(\"Building index with 100 trees...\")\n",
    "    index.build(100)\n",
    "    index.save(f\"{save_dir}/image_index.ann\")\n",
    "    \n",
    "    return index\n",
    "\n",
    "def find_similar(query_image_path, index, encoder, file_mapping, class_mapping, n_results=10):\n",
    "    query_embedding = encoder.get_embedding(query_image_path)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    \n",
    "    n_candidates = min(n_results * 3, len(file_mapping))\n",
    "    similar_idx, distances = index.get_nns_by_vector(\n",
    "        query_embedding, n_candidates, include_distances=True)\n",
    "    \n",
    "    filtered_results = []\n",
    "    seen_classes = set()\n",
    "    \n",
    "    for idx, dist in zip(similar_idx, distances):\n",
    "        class_name = class_mapping[idx]\n",
    "        if len(filtered_results) < n_results:\n",
    "            if class_name not in seen_classes:\n",
    "                filtered_results.append(file_mapping[idx])\n",
    "                seen_classes.add(class_name)\n",
    "    \n",
    "    while len(filtered_results) < n_results and similar_idx:\n",
    "        idx = similar_idx[len(filtered_results)]\n",
    "        filtered_results.append(file_mapping[idx])\n",
    "    \n",
    "    return filtered_results\n",
    "\n",
    "def calculate_map10(predictions, true_classes):\n",
    "    ap_scores = []\n",
    "    \n",
    "    for query_image, recommended_images in predictions.items():\n",
    "        if query_image not in true_classes:\n",
    "            continue\n",
    "            \n",
    "        true_class = true_classes[query_image]\n",
    "        \n",
    "        relevance = []\n",
    "        for rec_image in recommended_images[:10]:\n",
    "            rec_class = true_classes.get(rec_image)\n",
    "            relevance.append(1 if rec_class == true_class else 0)\n",
    "            \n",
    "        precision_at_k = []\n",
    "        relevant_count = 0\n",
    "        \n",
    "        for k, rel in enumerate(relevance, 1):\n",
    "            if rel == 1:\n",
    "                relevant_count += 1\n",
    "                precision_at_k.append(relevant_count / k)\n",
    "                \n",
    "        ap = sum(precision_at_k) / min(10, sum(relevance)) if sum(relevance) > 0 else 0\n",
    "        ap_scores.append(ap)\n",
    "    \n",
    "    map10 = sum(ap_scores) / len(ap_scores) if ap_scores else 0\n",
    "    return map10\n",
    "\n",
    "def evaluate_recommendations(test_dir, index, encoder, file_mapping, class_mapping, \n",
    "                           reverse_class_mapping, output_file=\"submission.csv\"):\n",
    "    test_path = Path(test_dir)\n",
    "    results = []\n",
    "    predictions = {}\n",
    "    \n",
    "    print(\"Generating recommendations for test images...\")\n",
    "    \n",
    "    for image_file in tqdm(list(test_path.glob(\"*.*\"))):\n",
    "        if image_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "            similar_images = find_similar(\n",
    "                str(image_file), index, encoder, file_mapping, \n",
    "                class_mapping, n_results=10\n",
    "            )\n",
    "            \n",
    "            if similar_images:\n",
    "                recs = \",\".join(similar_images)\n",
    "                results.append({\n",
    "                    'image': image_file.stem,\n",
    "                    'recs': f'\"{recs}\"'\n",
    "                })\n",
    "                predictions[image_file.stem] = similar_images\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSubmission saved to {output_file}\")\n",
    "    \n",
    "    test_classes = {}\n",
    "    for image_stem in predictions.keys():\n",
    "        class_dir = Path(str(test_path / image_stem)).parent.name\n",
    "        test_classes[image_stem] = class_dir\n",
    "    \n",
    "    if test_classes:\n",
    "        map10 = calculate_map10(predictions, {**reverse_class_mapping, **test_classes})\n",
    "        print(f\"\\nMAP@10: {map10:.4f}\")\n",
    "    \n",
    "    return predictions, map10\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Инициализация поисковой системы\n",
    "    encoder = VLMImageSearch()\n",
    "    \n",
    "    # Обработка датасета\n",
    "    dataset_path = \"dataset\"\n",
    "    embeddings_dict, file_mapping, class_mapping, reverse_class_mapping = process_dataset(\n",
    "        dataset_path, encoder\n",
    "    )\n",
    "    \n",
    "    # Создание индекса\n",
    "    index = build_index(embeddings_dict)\n",
    "    \n",
    "    # Оценка на тестовом наборе\n",
    "    test_dir = \"test\"\n",
    "    predictions, map10 = evaluate_recommendations(\n",
    "        test_dir, index, encoder, file_mapping, \n",
    "        class_mapping, reverse_class_mapping\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen-VL model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cpu!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Qwen-VL model...\")\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"Model loaded successfully on {device}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: {'input_ids': torch.Size([1, 4]), 'attention_mask': torch.Size([1, 4]), 'pixel_values': torch.Size([3552, 1176]), 'image_grid_thw': torch.Size([1, 3])}\n"
     ]
    }
   ],
   "source": [
    "# Загрузка изображения\n",
    "image = Image.open('0b9649438a916859.jpg').convert('RGB')\n",
    "\n",
    "# Предобработка\n",
    "inputs = processor(\n",
    "    text=[\"Describe this image.\"],\n",
    "    images=[image],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(device)\n",
    "\n",
    "print(\"Input shape:\", {k: v.shape for k, v in inputs.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Доступные атрибуты outputs:\n",
      "attentions\n",
      "clear\n",
      "copy\n",
      "fromkeys\n",
      "get\n",
      "hidden_states\n",
      "items\n",
      "keys\n",
      "logits\n",
      "loss\n",
      "move_to_end\n",
      "past_key_values\n",
      "pop\n",
      "popitem\n",
      "rope_deltas\n",
      "setdefault\n",
      "to_tuple\n",
      "update\n",
      "values\n",
      "\n",
      "Типы и размеры hidden states:\n",
      "Layer 0: shape = torch.Size([1, 4, 1536])\n",
      "Layer 1: shape = torch.Size([1, 4, 1536])\n",
      "Layer 2: shape = torch.Size([1, 4, 1536])\n",
      "Layer 3: shape = torch.Size([1, 4, 1536])\n",
      "Layer 4: shape = torch.Size([1, 4, 1536])\n",
      "Layer 5: shape = torch.Size([1, 4, 1536])\n",
      "Layer 6: shape = torch.Size([1, 4, 1536])\n",
      "Layer 7: shape = torch.Size([1, 4, 1536])\n",
      "Layer 8: shape = torch.Size([1, 4, 1536])\n",
      "Layer 9: shape = torch.Size([1, 4, 1536])\n",
      "Layer 10: shape = torch.Size([1, 4, 1536])\n",
      "Layer 11: shape = torch.Size([1, 4, 1536])\n",
      "Layer 12: shape = torch.Size([1, 4, 1536])\n",
      "Layer 13: shape = torch.Size([1, 4, 1536])\n",
      "Layer 14: shape = torch.Size([1, 4, 1536])\n",
      "Layer 15: shape = torch.Size([1, 4, 1536])\n",
      "Layer 16: shape = torch.Size([1, 4, 1536])\n",
      "Layer 17: shape = torch.Size([1, 4, 1536])\n",
      "Layer 18: shape = torch.Size([1, 4, 1536])\n",
      "Layer 19: shape = torch.Size([1, 4, 1536])\n",
      "Layer 20: shape = torch.Size([1, 4, 1536])\n",
      "Layer 21: shape = torch.Size([1, 4, 1536])\n",
      "Layer 22: shape = torch.Size([1, 4, 1536])\n",
      "Layer 23: shape = torch.Size([1, 4, 1536])\n",
      "Layer 24: shape = torch.Size([1, 4, 1536])\n",
      "Layer 25: shape = torch.Size([1, 4, 1536])\n",
      "Layer 26: shape = torch.Size([1, 4, 1536])\n",
      "Layer 27: shape = torch.Size([1, 4, 1536])\n",
      "Layer 28: shape = torch.Size([1, 4, 1536])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "print(\"\\nДоступные атрибуты outputs:\")\n",
    "for attr in dir(outputs):\n",
    "    if not attr.startswith('_'):\n",
    "        print(attr)\n",
    "        \n",
    "print(\"\\nТипы и размеры hidden states:\")\n",
    "for i, hidden_states in enumerate(outputs.hidden_states):\n",
    "    print(f\"Layer {i}: shape = {hidden_states.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 1: Импорты и настройка\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Ячейка 2: Загрузка модели и процессора\n",
    "def load_model_and_processor():\n",
    "    print(\"Loading Qwen-VL model...\")\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded successfully on {device}!\")\n",
    "    return model, processor, device\n",
    "\n",
    "# Ячейка 3: Тестирование на одном изображении\n",
    "def process_single_image(model, processor, device, image_path):\n",
    "    # Загрузка изображения\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Предобработка\n",
    "    inputs = processor(\n",
    "        text=[\"Describe this image.\"],\n",
    "        images=[image],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    print(\"Input shape:\", {k: v.shape for k, v in inputs.items()})\n",
    "    return inputs\n",
    "\n",
    "# Ячейка 4: Получение и анализ hidden states\n",
    "def get_hidden_states(model, inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        \n",
    "    print(\"\\nДоступные атрибуты outputs:\")\n",
    "    for attr in dir(outputs):\n",
    "        if not attr.startswith('_'):\n",
    "            print(attr)\n",
    "            \n",
    "    print(\"\\nТипы и размеры hidden states:\")\n",
    "    for i, hidden_states in enumerate(outputs.encoder_hidden_states):\n",
    "        print(f\"Layer {i}: shape = {hidden_states.shape}\")\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Ячейка 5: Извлечение эмбеддинга\n",
    "def extract_embedding(outputs):\n",
    "    # Берем последний слой hidden states\n",
    "    last_hidden_states = outputs.encoder_hidden_states[-1]\n",
    "    print(\"\\nРазмерность последнего слоя:\", last_hidden_states.shape)\n",
    "    \n",
    "    # Усредняем по токенам\n",
    "    embedding = last_hidden_states.mean(dim=1).squeeze().cpu().numpy()\n",
    "    print(\"Размерность эмбеддинга:\", embedding.shape)\n",
    "    \n",
    "    # Нормализация\n",
    "    embedding = embedding / np.linalg.norm(embedding)\n",
    "    print(\"Норма эмбеддинга:\", np.linalg.norm(embedding))\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Ячейка 6: Полный пайплайн для тестирования\n",
    "def test_embedding_pipeline(image_path):\n",
    "    # Загрузка модели\n",
    "    model, processor, device = load_model_and_processor()\n",
    "    \n",
    "    # Обработка изображения\n",
    "    inputs = process_single_image(model, processor, device, image_path)\n",
    "    \n",
    "    # Получение hidden states\n",
    "    outputs = get_hidden_states(model, inputs)\n",
    "    \n",
    "    # Извлечение эмбеддинга\n",
    "    embedding = extract_embedding(outputs)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Ячейка 7: Запуск тестирования\n",
    "image_path = \"dataset/Accordion/01cc22eb34653a82.jpg\"  # Укажите путь к тестовому изображению\n",
    "embedding = test_embedding_pipeline(image_path)\n",
    "\n",
    "# Ячейка 8: Визуализация эмбеддинга\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(embedding)\n",
    "plt.title('Embedding Values')\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(embedding, bins=50)\n",
    "plt.title('Embedding Distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
