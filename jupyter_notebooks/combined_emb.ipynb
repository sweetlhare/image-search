{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89803c0-0be2-4b50-a5dd-106ac7271fb0",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a622ba80-3311-4698-a53f-89b50a665475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from annoy import AnnoyIndex\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd9b68e-56fe-40dd-ad24-eadee105d757",
   "metadata": {},
   "source": [
    "### Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6352915b-1ab5-49c4-b354-5178f1bb43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder:\n",
    "    def __init__(self):\n",
    "        print(\"Loading ConvNeXT model...\")\n",
    "        self.model = models.convnext_large(pretrained=True)\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(236, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        print(f\"Model loaded successfully on {self.device}!\")\n",
    "\n",
    "    def get_embedding(self, image_path):\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.transform(image).unsqueeze(0)\n",
    "            image = image.to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embedding = self.model(image)\n",
    "                \n",
    "            embedding = embedding.squeeze().cpu().numpy()\n",
    "            embedding = embedding / np.linalg.norm(embedding)\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "    def get_combined_embedding(self, image_path, description, text_model):\n",
    "        # Визуальный эмбеддинг\n",
    "        image_embedding = self.get_embedding(image_path)\n",
    "        if image_embedding is None:\n",
    "            return None\n",
    "        # Текстовый эмбеддинг\n",
    "        text_embedding = text_model.encode(description, convert_to_tensor=True, normalize_embeddings=True).cpu().numpy()\n",
    "        # Конкатенация с нормализацией \n",
    "        combined_embedding = np.concatenate((image_embedding, text_embedding))\n",
    "        combined_embedding = combined_embedding / np.linalg.norm(combined_embedding)\n",
    "        return combined_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f04704c-6f53-4bc0-9ae5-71e02b56f8ce",
   "metadata": {},
   "source": [
    "### Создание экземпляров энкодера и моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0b396db-22e8-4115-94bc-2af6271a7290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ConvNeXT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moo/PycharmProjects/jupyter-venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/moo/PycharmProjects/jupyter-venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Large_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cuda!\n",
      "Loading BLIP model for image captioning...\n",
      "BLIP model loaded successfully!\n",
      "Loading SentenceTransformer model for text embeddings...\n",
      "SentenceTransformer model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Создаем экземпляр энкодера изображений\n",
    "encoder = ImageEncoder()\n",
    "\n",
    "# Загружаем модель BLIP для генерации описаний\n",
    "print(\"Loading BLIP model for image captioning...\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "caption_model = caption_model.to(encoder.device)\n",
    "print(\"BLIP model loaded successfully!\")\n",
    "\n",
    "# Загружаем модель SentenceTransformer для текстовых эмбеддингов\n",
    "print(\"Loading SentenceTransformer model for text embeddings...\")\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2', device=encoder.device)\n",
    "print(\"SentenceTransformer model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c7eed4-6403-4b1e-9e11-ccfad54d6365",
   "metadata": {},
   "source": [
    "### Генерация описаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a05fbb4-8f2a-4039-88df-3ae5ae0b4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(image_path, processor, model):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(encoder.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs)\n",
    "        description = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        return description\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating description for {image_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a6a4d-ce08-48d2-8fa9-85499d9ff0cc",
   "metadata": {},
   "source": [
    "### Прогон описаний для датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d818d0f2-68a9-4d62-9c15-1f2843d96829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating descriptions for images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                     | 0/105 [00:00<?, ?it/s]/home/moo/PycharmProjects/jupyter-venv/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [21:51<00:00, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All descriptions saved to ./descriptions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_image_folder(folder_path, processor, model, output_file):\n",
    "    folder_path = Path(folder_path)\n",
    "    descriptions = {}\n",
    "    print(\"Generating descriptions for images...\")\n",
    "    \n",
    "    # Обработка изображений в корневой папке\n",
    "    for image_file in folder_path.glob(\"*.*\"):\n",
    "        if image_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp', '.png']:\n",
    "            description = generate_description(str(image_file), processor, model)\n",
    "            if description:\n",
    "                descriptions[image_file.name] = description  # Используем image_file.name в качестве ключа\n",
    "\n",
    "    # Обработка изображений в поддиректориях\n",
    "    for class_dir in tqdm(list(folder_path.iterdir())):\n",
    "        if class_dir.is_dir():\n",
    "            for image_file in class_dir.glob(\"*.*\"):\n",
    "                if image_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp', '.png']:\n",
    "                    description = generate_description(str(image_file), processor, model)\n",
    "                    if description:\n",
    "                        descriptions[image_file.name] = description  # Используем image_file.name в качестве ключа\n",
    "\n",
    "    # Сохраняем описания в файл JSON\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(descriptions, f)\n",
    "    print(f\"\\nAll descriptions saved to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "dataset_path = \"/home/moo/Downloads/train_dataset_train_data_rkn/train_data_rkn/train\"\n",
    "descriptions_file = \"./descriptions.json\"\n",
    "process_image_folder(dataset_path, processor, caption_model, output_file=descriptions_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd5bf4d-59ba-4d57-b311-67536eee89cc",
   "metadata": {},
   "source": [
    "### Обработка и получение эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf6d75-3411-4202-b3b6-4d0c0d86c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_path, encoder, text_model, descriptions_file, save_dir=\"./data\"):\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    # Загрузка описаний из файла\n",
    "    with open(descriptions_file, 'r') as f:\n",
    "        descriptions_dict = json.load(f)\n",
    "    \n",
    "    embeddings_dict = {}\n",
    "    file_mapping = {}\n",
    "    class_mapping = {}\n",
    "    reverse_class_mapping = {}  # Для быстрого получения класса по имени файла\n",
    "    class_stats = defaultdict(int)\n",
    "    \n",
    "    idx = 0\n",
    "    print(\"Processing dataset with combined embeddings...\")\n",
    "    \n",
    "    for class_dir in tqdm(list(dataset_path.iterdir())):\n",
    "        if class_dir.is_dir():\n",
    "            class_name = class_dir.name\n",
    "            for image_file in class_dir.glob(\"*.*\"):\n",
    "                if image_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                    image_path = str(image_file)\n",
    "                    description = descriptions_dict.get(image_path)\n",
    "                    if description is None:\n",
    "                        print(f\"No description found for {image_path}, skipping.\")\n",
    "                        continue\n",
    "                    embedding = encoder.get_combined_embedding(image_path, description, text_model)\n",
    "                    if embedding is not None:\n",
    "                        embeddings_dict[idx] = embedding\n",
    "                        file_mapping[idx] = image_file.stem\n",
    "                        class_mapping[idx] = class_name\n",
    "                        reverse_class_mapping[image_file.stem] = class_name\n",
    "                        class_stats[class_name] += 1\n",
    "                        idx += 1\n",
    "    \n",
    "    print(f\"\\nTotal images processed: {idx}\")\n",
    "    print(\"\\nClass distribution:\")\n",
    "    for class_name, count in class_stats.items():\n",
    "        print(f\"{class_name}: {count} images\")\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with open(f\"{save_dir}/processed_data_combined.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': embeddings_dict,\n",
    "            'file_mapping': file_mapping,\n",
    "            'class_mapping': class_mapping,\n",
    "            'reverse_class_mapping': reverse_class_mapping,\n",
    "            'class_stats': dict(class_stats)\n",
    "        }, f)\n",
    "    \n",
    "    return embeddings_dict, file_mapping, class_mapping, reverse_class_mapping\n",
    "\n",
    "# Обработка датасета\n",
    "embeddings_dict, file_mapping, class_mapping, reverse_class_mapping = process_dataset(\n",
    "    dataset_path, encoder, text_model, descriptions_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e391214-0fc8-442f-b283-fe81f2c795f9",
   "metadata": {},
   "source": [
    "### Создание и сохранение индекса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40cbe3-c04c-44d4-b425-5e5cea929db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(embeddings_dict, save_dir=\"./data_s2\"):\n",
    "    first_embedding = next(iter(embeddings_dict.values()))\n",
    "    embedding_dim = len(first_embedding)\n",
    "    \n",
    "    index = AnnoyIndex(embedding_dim, 'angular')\n",
    "    \n",
    "    print(\"Building index...\")\n",
    "    for idx, embedding in embeddings_dict.items():\n",
    "        index.add_item(idx, embedding)\n",
    "    \n",
    "    print(\"Building index with 100 trees...\")\n",
    "    index.build(100)\n",
    "    index.save(f\"{save_dir}/image_index.ann\")\n",
    "    \n",
    "    return index\n",
    "\n",
    "# Создание индекса\n",
    "index = build_index(embeddings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb44e24-67a1-41b6-9b45-7f4310a81669",
   "metadata": {},
   "source": [
    "### Поиск похожих изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc55452-f5a7-46a5-b86f-f7eb4d7f6891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(query_image_path, description, index, encoder, text_model, file_mapping, class_mapping, n_results=10):\n",
    "    query_embedding = encoder.get_combined_embedding(query_image_path, description, text_model)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    \n",
    "    n_candidates = min(n_results * 3, len(file_mapping))\n",
    "    similar_idx, distances = index.get_nns_by_vector(\n",
    "        query_embedding, n_candidates, include_distances=True)\n",
    "    \n",
    "    filtered_results = []\n",
    "    seen_classes = set()\n",
    "    \n",
    "    for idx, dist in zip(similar_idx, distances):\n",
    "        class_name = class_mapping[idx]\n",
    "        if len(filtered_results) < n_results:\n",
    "            if class_name not in seen_classes:\n",
    "                filtered_results.append(file_mapping[idx])\n",
    "                seen_classes.add(class_name)\n",
    "    \n",
    "    while len(filtered_results) < n_results and len(similar_idx) > len(filtered_results):\n",
    "        idx = similar_idx[len(filtered_results)]\n",
    "        filtered_results.append(file_mapping[idx])\n",
    "    \n",
    "    return filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939cfdd8-1eaf-4270-80be-b6c8ad327f8f",
   "metadata": {},
   "source": [
    "### map10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc6b8a8-7400-49f3-abb9-f00dcbf10130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map10(predictions, dataset_path, test_class_mapping):\n",
    "    \"\"\"\n",
    "    Правильный расчет MAP@10 на основе классов изображений.\n",
    "\n",
    "    Args:\n",
    "        predictions: dict {query_image: [recommended_images]}\n",
    "        dataset_path: путь к тренировочному датасету\n",
    "        test_class_mapping: dict {image_name: class_name} для тестовых изображений\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    from collections import defaultdict\n",
    "\n",
    "    dataset_path = Path(dataset_path)\n",
    "    ap_scores = []\n",
    "    class_performances = defaultdict(list)\n",
    "\n",
    "    # Создаем маппинг классов для тренировочных изображений\n",
    "    train_class_mapping = {}\n",
    "    for class_dir in dataset_path.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            class_name = class_dir.name\n",
    "            for img_path in class_dir.glob(\"*.*\"):\n",
    "                if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                    train_class_mapping[img_path.name] = class_name\n",
    "\n",
    "    # Проверка наличия изображений в маппингах\n",
    "    if not train_class_mapping:\n",
    "        print(\"Ошибка: train_class_mapping пуст.\")\n",
    "    if not test_class_mapping:\n",
    "        print(\"Ошибка: test_class_mapping пуст.\")\n",
    "\n",
    "    # Для каждого тестового изображения\n",
    "    for query_image, recommended_images in predictions.items():\n",
    "        query_class = test_class_mapping.get(query_image)\n",
    "        if query_class is None:\n",
    "            print(f\"Класс не найден для тестового изображения: {query_image}\")\n",
    "            continue\n",
    "\n",
    "        # Считаем релевантность для каждой позиции\n",
    "        relevance = []\n",
    "        for rec_image in recommended_images[:10]:\n",
    "            rec_class = train_class_mapping.get(rec_image)\n",
    "            is_relevant = 1 if rec_class == query_class else 0\n",
    "            relevance.append(is_relevant)\n",
    "\n",
    "        # Считаем AP@10\n",
    "        precision_at_k = []\n",
    "        num_relevant = 0\n",
    "\n",
    "        for k, is_relevant in enumerate(relevance, 1):\n",
    "            if is_relevant:\n",
    "                num_relevant += 1\n",
    "                precision_at_k.append(num_relevant / k)\n",
    "\n",
    "        if num_relevant > 0:\n",
    "            ap = sum(precision_at_k) / num_relevant\n",
    "            ap_scores.append(ap)\n",
    "            class_performances[query_class].append(ap)\n",
    "        else:\n",
    "            ap_scores.append(0)\n",
    "            class_performances[query_class].append(0)\n",
    "\n",
    "    # Считаем общий MAP@10\n",
    "    map10 = np.mean(ap_scores) if ap_scores else 0\n",
    "\n",
    "    # Считаем MAP@10 по классам\n",
    "    class_map = {cls: np.mean(scores) for cls, scores in class_performances.items()}\n",
    "\n",
    "    return map10, class_map\n",
    "\n",
    "import csv\n",
    "\n",
    "def load_test_class_mapping(mapping_file):\n",
    "    test_class_mapping = {}\n",
    "    with open(mapping_file, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            test_class_mapping[row['image_name']] = row['class_name']\n",
    "    return test_class_mapping\n",
    "\n",
    "# Загрузка маппинга\n",
    "valid_mapping_file = '/home/moo/Downloads/train_dataset_train_data_rkn/train_data_rkn/valid_mapping.csv'\n",
    "test_class_mapping = load_test_class_mapping(valid_mapping_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6625dd5-d747-4ddf-a6bb-7840e8e4a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(test_dir, index, encoder, text_model, descriptions_file, file_mapping, class_mapping, dataset_path, test_class_mapping, output_file=\"submission_s2.csv\"):\n",
    "    \"\"\"\n",
    "    Генерация рекомендаций и подсчет метрик.\n",
    "    \"\"\"\n",
    "    # Загрузка описаний для тестовых изображений\n",
    "    with open(descriptions_file, 'r') as f:\n",
    "        test_descriptions_dict = json.load(f)\n",
    "\n",
    "    test_path = Path(test_dir)\n",
    "    results = []\n",
    "    predictions = {}\n",
    "\n",
    "    print(\"Generating recommendations for test images...\")\n",
    "    for image_file in tqdm(list(test_path.glob(\"*.*\"))):\n",
    "        if image_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "            image_path = str(image_file)\n",
    "            description = test_descriptions_dict.get(image_file.name)\n",
    "            if description is None:\n",
    "                print(f\"No description found for {image_file.name}, skipping.\")\n",
    "                continue\n",
    "            similar_images = find_similar(\n",
    "                image_path, description, index, encoder, text_model, file_mapping, \n",
    "                class_mapping, n_results=10\n",
    "            )\n",
    "            print(similar_images)\n",
    "            if similar_images:\n",
    "                recs = \",\".join(similar_images)\n",
    "                results.append({\n",
    "                    'image': image_file.name,  # Используем image_file.name\n",
    "                    'recs': f'\"{recs}\"'\n",
    "                })\n",
    "                predictions[image_file.name] = similar_images  # Используем image_file.name\n",
    "\n",
    "    # Сохраняем результаты\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSubmission saved to {output_file}\")\n",
    "\n",
    "    # Считаем метрики\n",
    "    map10, class_map = calculate_map10(predictions, dataset_path, test_class_mapping)\n",
    "\n",
    "    print(f\"\\nOverall MAP@10: {map10:.4f}\")\n",
    "    print(\"\\nMAP@10 by class:\")\n",
    "    for class_name, class_score in sorted(class_map.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{class_name}: {class_score:.4f}\")\n",
    "\n",
    "    return predictions, map10, class_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfeb7ba-648d-435d-b8c4-43422262d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_failure_cases(predictions, dataset_path, test_path, class_map):\n",
    "    \"\"\"\n",
    "    Анализ классов с наименьшим значением MAP@10.\n",
    "    \"\"\"\n",
    "    worst_classes = sorted(class_map.items(), key=lambda x: x[1])[:5]\n",
    "    print(\"\\nAnalyzing worst performing classes:\")\n",
    "    \n",
    "    dataset_path = Path(dataset_path)\n",
    "    test_path = Path(test_path)\n",
    "    \n",
    "    # Создаем маппинг классов для тренировочных изображений\n",
    "    train_class_mapping = {}\n",
    "    for class_dir in dataset_path.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            class_name = class_dir.name\n",
    "            for img_path in class_dir.glob(\"*.*\"):\n",
    "                if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                    train_class_mapping[img_path.stem] = class_name\n",
    "    \n",
    "    # Создаем маппинг классов для тестовых изображений\n",
    "    test_class_mapping = {}\n",
    "    for class_dir in test_path.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            class_name = class_dir.name\n",
    "            for img_path in class_dir.glob(\"*.*\"):\n",
    "                if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                    test_class_mapping[img_path.stem] = class_name\n",
    "    \n",
    "    for class_name, score in worst_classes:\n",
    "        print(f\"\\nClass: {class_name}, MAP@10: {score:.4f}\")\n",
    "        \n",
    "        # Находим примеры ошибок для этого класса\n",
    "        for query_image, recs in predictions.items():\n",
    "            query_class = test_class_mapping.get(query_image)\n",
    "            if query_class == class_name:\n",
    "                print(f\"\\nQuery image: {query_image}\")\n",
    "                print(\"Top 5 recommendations and their classes:\")\n",
    "                for i, rec in enumerate(recs[:5], 1):\n",
    "                    rec_class = train_class_mapping.get(rec, \"unknown\")\n",
    "                    print(f\"{i}. {rec} (class: {rec_class})\")\n",
    "                # Выводим только один пример для каждого класса\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2334fe4-c424-49bb-8237-dd28f248a666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbce1f4-ef05-4b74-88cd-a8a0c91344e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4f000-0e7d-4e23-b1a4-bd05157706d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/home/moo/Downloads/train_dataset_train_data_rkn/train_data_rkn/validd\"\n",
    "dataset_path = \"/home/moo/Downloads/train_dataset_train_data_rkn/train_data_rkn/train\"\n",
    "test_descriptions_file = \"./test_descriptions.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9bf7d2-a2c2-4a33-bc79-320cfe9c6db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Описания для тестовых изображений\n",
    "process_image_folder(test_dir, processor, caption_model, output_file=test_descriptions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6a2d4-172f-43c0-905c-4977c5dd5c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, map10, class_map = evaluate_recommendations(\n",
    "    test_dir=test_dir,\n",
    "    index=index,\n",
    "    encoder=encoder,\n",
    "    text_model=text_model,\n",
    "    descriptions_file=test_descriptions_file,\n",
    "    file_mapping=file_mapping,\n",
    "    class_mapping=class_mapping,\n",
    "    dataset_path=dataset_path,\n",
    "    test_class_mapping=test_class_mapping\n",
    ")\n",
    "\n",
    "# Анализ проблемных случаев\n",
    "analyze_failure_cases(predictions, dataset_path, test_dir, class_map)\n",
    "\n",
    "# Визуализация распределения MAP@10 по классам\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(list(class_map.values()), bins=20, kde=False)\n",
    "plt.title('Distribution of MAP@10 across classes')\n",
    "plt.xlabel('MAP@10')\n",
    "plt.ylabel('Number of classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2da86bb-70c7-4ff4-b158-be5a3cb200ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b4bb7-3fff-4d13-b038-6935487016a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
