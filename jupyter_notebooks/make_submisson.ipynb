{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327fb942-0cde-447a-80f1-1feff208c803",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95db707-bc0d-462f-b056-1045dc52262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from annoy import AnnoyIndex\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d851c-0514-458d-a59c-1ae6cbd5a642",
   "metadata": {},
   "source": [
    "### Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ee9d151-e449-4150-bb4a-87d32b276d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder:\n",
    "    def __init__(self):\n",
    "        print(\"Loading ConvNeXT model...\")\n",
    "        self.model = models.convnext_large(pretrained=True)\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(236, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        print(f\"Model loaded successfully on {self.device}!\")\n",
    "\n",
    "    def get_embedding(self, image_path):\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.transform(image).unsqueeze(0)\n",
    "            image = image.to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embedding = self.model(image)\n",
    "                \n",
    "            embedding = embedding.squeeze().cpu().numpy()\n",
    "            embedding = embedding / np.linalg.norm(embedding)\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "428ee7e7-8c9f-49e1-9c9b-a14e9b86bd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ConvNeXT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moo/PycharmProjects/jupyter-venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/moo/PycharmProjects/jupyter-venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Large_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cuda!\n"
     ]
    }
   ],
   "source": [
    "encoder = ImageEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2862393e-5b08-46f1-b885-0090b08aad07",
   "metadata": {},
   "source": [
    "### Обработка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c61c14-31d6-4c43-8409-e513ff661150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_path, encoder, save_dir=\"./data\"):\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    embeddings_dict = {}\n",
    "    file_mapping = {}\n",
    "    class_mapping = {}\n",
    "    reverse_class_mapping = {} \n",
    "    class_stats = defaultdict(int)\n",
    "    idx = 0\n",
    "    print(\"Processing dataset...\")\n",
    "    \n",
    "    for class_dir in tqdm(list(dataset_path.iterdir())):\n",
    "        if class_dir.is_dir():\n",
    "            class_name = class_dir.name\n",
    "            for image_file in class_dir.glob(\"*.*\"):\n",
    "                if image_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                    embedding = encoder.get_embedding(str(image_file))\n",
    "                    if embedding is not None:\n",
    "                        embeddings_dict[idx] = embedding\n",
    "                        file_mapping[idx] = image_file.name\n",
    "                        class_mapping[idx] = class_name\n",
    "                        reverse_class_mapping[image_file.name] = class_name\n",
    "                        class_stats[class_name] += 1\n",
    "                        idx += 1\n",
    "    \n",
    "    # print(f\"\\nTotal images processed: {idx}\")\n",
    "    # print(\"\\nClass distribution:\")\n",
    "    # for class_name, count in class_stats.items():\n",
    "    #     print(f\"{class_name}: {count} images\")\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with open(f\"{save_dir}/processed_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': embeddings_dict,\n",
    "            'file_mapping': file_mapping,\n",
    "            'class_mapping': class_mapping,\n",
    "            'reverse_class_mapping': reverse_class_mapping,\n",
    "            'class_stats': dict(class_stats)\n",
    "        }, f)\n",
    "    \n",
    "    return embeddings_dict, file_mapping, class_mapping, reverse_class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9499b4d8-7fae-4c22-9755-39ed283dd3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [08:18<00:00,  4.75s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/moo/Downloads/train_dataset_train_data_rkn/train_data_rkn/dataset\"  # Путь к датасету\n",
    "embeddings_dict, file_mapping, class_mapping, reverse_class_mapping = process_dataset(dataset_path, encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ddd75-017e-4eb9-8fd7-5aae2be887cd",
   "metadata": {},
   "source": [
    "### Индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a084f60-fc0a-40ca-b538-202de040f521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index with 100 trees...\n"
     ]
    }
   ],
   "source": [
    "def build_index(embeddings_dict, save_dir=\"./data\"):\n",
    "    first_embedding = next(iter(embeddings_dict.values()))\n",
    "    embedding_dim = len(first_embedding)\n",
    "    \n",
    "    index = AnnoyIndex(embedding_dim, 'angular')\n",
    "    \n",
    "    for idx, embedding in embeddings_dict.items():\n",
    "        index.add_item(idx, embedding)\n",
    "    \n",
    "    print(\"Building index with 100 trees...\")\n",
    "    index.build(100)\n",
    "    index.save(f\"{save_dir}/image_index.ann\")\n",
    "    \n",
    "    return index\n",
    "\n",
    "index = build_index(embeddings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d817a83-9286-45aa-b53e-543411f509f3",
   "metadata": {},
   "source": [
    "### Поиск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e461412-e9af-40d7-af89-7ada68bbe6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(query_image_path, index, encoder, file_mapping, class_mapping, n_results=10):\n",
    "    query_embedding = encoder.get_embedding(query_image_path)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    \n",
    "    n_candidates = 30\n",
    "    similar_idx, distances = index.get_nns_by_vector(\n",
    "        query_embedding, n_candidates, include_distances=True)\n",
    "    \n",
    "    candidate_classes = []\n",
    "    candidate_files = []\n",
    "    candidate_distances = []\n",
    "    query_image_name = os.path.basename(query_image_path)\n",
    "    \n",
    "    for idx, dist in zip(similar_idx, distances):\n",
    "        candidate_file = file_mapping[idx]\n",
    "        if candidate_file == query_image_name:\n",
    "            continue\n",
    "        candidate_classes.append(class_mapping[idx])\n",
    "        candidate_files.append(candidate_file)\n",
    "        candidate_distances.append(dist)\n",
    "    \n",
    "    if not candidate_files:\n",
    "        return []\n",
    "    \n",
    "    distances = candidate_distances\n",
    "    # Проверка 1\n",
    "    similarity_threshold = 0.1\n",
    "    if distances[0] < similarity_threshold:\n",
    "        assigned_class = candidate_classes[0]\n",
    "        class_images = [f for f, cls in zip(candidate_files, candidate_classes) if cls == assigned_class]\n",
    "        similar_images = class_images[:n_results]\n",
    "    else:\n",
    "        # Проверка 2\n",
    "        class_counts = {}\n",
    "        for cls in candidate_classes[:10]:\n",
    "            class_counts[cls] = class_counts.get(cls, 0) + 1\n",
    "        \n",
    "        most_common_class = max(class_counts, key=class_counts.get)\n",
    "        if class_counts[most_common_class] >= 6:\n",
    "            assigned_class = most_common_class\n",
    "            class_images = [f for f, cls in zip(candidate_files, candidate_classes) if cls == assigned_class]\n",
    "            similar_images = class_images[:n_results]\n",
    "        else:\n",
    "            similar_images = candidate_files[:n_results]\n",
    "\n",
    "    if len(similar_images) < n_results:\n",
    "        additional_images = [f for f in candidate_files if f not in similar_images]\n",
    "        similar_images.extend(additional_images[:n_results - len(similar_images)])\n",
    "    \n",
    "    return similar_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfec08-9669-402d-a214-1301c277e4b7",
   "metadata": {},
   "source": [
    "### Генерация предикта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ed47ce1-4c9a-4606-a102-1a7680f41185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(test_dir, index, encoder, file_mapping, class_mapping, output_file=\"submission.csv\"):\n",
    "\n",
    "    test_path = Path(test_dir)\n",
    "    results = []\n",
    "    print(\"Generating recommendations for test images...\")\n",
    "    for image_file in list(test_path.rglob(\"*.*\")):\n",
    "        if image_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "            query_image_path = str(image_file)\n",
    "            similar_images = find_similar(\n",
    "                query_image_path, index, encoder, file_mapping, class_mapping, n_results=10)\n",
    "            \n",
    "            if similar_images:\n",
    "                recs = \",\".join(similar_images)\n",
    "                results.append({\n",
    "                    'image': image_file.name,\n",
    "                    'recs': f'\"{recs}\"'\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nRecommendations saved to {output_file}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7b7ac96-6132-48ad-a8fd-b54303527d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating recommendations for test images...\n",
      "\n",
      "Recommendations saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "test_dir = \"/home/moo/Downloads/train_dataset_train_data_rkn/train_data_rkn/test\"\n",
    "\n",
    "results = generate_recommendations(\n",
    "    test_dir=test_dir,\n",
    "    index=index,\n",
    "    encoder=encoder,\n",
    "    file_mapping=file_mapping,\n",
    "    class_mapping=class_mapping,\n",
    "    output_file=\"submission.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc2613-ca0b-4b2c-ac24-34f199729f29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
